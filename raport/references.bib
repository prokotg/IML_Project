@data{DVN/LM4OWF_2019,
author = {Solt, Frederick},
publisher = {Harvard Dataverse},
title = {{The Standardized World Income Inequality Database, Versions 8-9}},
year = {2019},
version = {V5},
doi = {10.7910/DVN/LM4OWF},
url = {https://doi.org/10.7910/DVN/LM4OWF}
}

@inproceedings{10.1145/584792.584890,
author = {Hamerly, Greg and Elkan, Charles},
title = {Alternatives to the K-Means Algorithm That Find Better Clusterings},
year = {2002},
isbn = {1581134924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584792.584890},
doi = {10.1145/584792.584890},
abstract = {We investigate here the behavior of the standard k-means clustering algorithm and several alternatives to it: the k-harmonic means algorithm due to Zhang and colleagues, fuzzy k-means, Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to find which aspects of these algorithms contribute to finding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a unified framework that introduces separate cluster membership and data weight functions. We then show that the algorithms do behave very differently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the k-harmonic means method is superior. Having a soft membership function is essential for finding high-quality clusterings, but having a non-constant data weight function is useful also.},
booktitle = {Proceedings of the Eleventh International Conference on Information and Knowledge Management},
pages = {600–607},
numpages = {8},
keywords = {unsupervised classification, k-harmonic means, k-means, clustering quality},
location = {McLean, Virginia, USA},
series = {CIKM '02}
}

@article{https://doi.org/10.1002/widm.16,
author = {Lingras, Pawan and Peters, Georg},
title = {Rough clustering},
journal = {WIREs Data Mining and Knowledge Discovery     },
volume = {1},
number = {1},
pages = {64-72},
doi = {https://doi.org/10.1002/widm.16},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.16},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.16},
abstract = {Abstract Traditional clustering partitions a group of objects into a number of nonoverlapping sets based on a similarity measure. In real world, the boundaries of these sets or clusters may not be clearly defined. Some of the objects may be almost equidistant from the center of multiple clusters. Traditional set theory mandates that these objects be assigned to a single cluster. Rough set theory can be used to represent the overlapping clusters. Rough sets provide more flexible representation than conventional sets, at the same time they are less descriptive than the fuzzy sets. This paper describes the basic concept of rough clustering based on k-means, genetic algorithms, Kohonen self-organizing maps, and support vector clustering. The discussion also includes a review of rough cluster validity measures, and applications of rough clustering to such diverse areas as forestry, medicine, medical imaging, web mining, super markets, and traffic engineering. © 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 64-72 DOI: 10.1002/widm.16 This article is categorized under: Technologies > Computational Intelligence Technologies > Machine Learning Technologies > Structure Discovery and Clustering},
year = {2011}
}

